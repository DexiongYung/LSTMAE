import torch
from torch import Tensor


def char_to_index(char: str, allowed_chars: str) -> int:
    idx = allowed_chars.find(char)
    if idx == -1: raise Exception(f"letter {char} is not permitted.")
    return idx


def index_to_char(idx: int, allowed_chars: str) -> int:
    return allowed_chars[idx]


def string_to_tensor(string: str, allowed_chars: str) -> list:
    tensor = torch.zeros(len(string), 1, len(allowed_chars))
    for i, char in enumerate(string):
        tensor[i, 0, char_to_index(char, allowed_chars)] = 1
    return tensor

def strings_to_tensor(names: list, max_name_len: int, allowed_letters: str):
    """
    Turn a list of name strings into a tensor of one-hot letter vectors
    of shape: <max_name_len x len(names) x n_letters>

    All names are padded with '<pad_character>' such that they have the length: desired_len
    names: List of names to converted to a one-hot-encded vector
    max_name_len: The max name length allowed
    """
    tensor = torch.zeros(max_name_len, len(names), len(allowed_letters))
    for i_name, name in enumerate(names):
        for i_char, letter in enumerate(name):
            tensor[i_char][i_name][allowed_letters.find(letter)] = 1
    return tensor

def targetTensor(name: str, allowed_chars: str, eos: str):
    letter_indexes = [allowed_chars.find(name[li]) for li in range(1, len(name))]
    letter_indexes.append(allowed_chars.find(eos)) # EOS
    return torch.LongTensor(letter_indexes)

def to_rnn_tensor(tensor: Tensor, letter_count: int) -> list:
    """
    Turn the tensor generated by strings_to_tensor into a one hot version to be passed to the RNN
    """
    input_sz, batch_sz = tensor.shape
    encoded = torch.zeros(input_sz, batch_sz, letter_count)
    for i in range(input_sz):
        for j in range(batch_sz):
            encoded[i, j, int(tensor[i, j])] = 1
    return encoded


def strings_to_index_tensor(strings: list, max_string_len: int, allowed_chars: str, index_function) -> list:
    """
    Turn a list of strings into a tensor of shape: <max_string_len x batch_size (length of strings)>.
    index_function should be a function that converts a character into an appropriate index.
    Example: strings: ["012","9 ."], max_string_len: 4,
            => torch.tensor([[0,9],[1,10],[2,11],[10,10]])
    """
    tensor = torch.zeros(max_string_len, len(strings))
    for i_s, s in enumerate(strings):
        for i_char, char in enumerate(s):
            tensor[i_char][i_s] = index_function(char, allowed_chars)
    return tensor


def int_to_tensor(index: int, allowed_chars: str) -> list:
    tensor = torch.zeros([1, len(allowed_chars)], dtype=torch.long)
    tensor[:, index] = 1
    return tensor


def pad_string(original: str, desired_len: int, pad_character: str, pre_pad: bool = True):
    """
    Returns the padded version of the original string to length: desired_len
    original: The string to be padded
    desired_len: The length of the new string
    pad_character: The character used to pad the original
    """
    if pre_pad:
        return (str(pad_character) * (desired_len - len(original))) + original
    else:
        return original + (str(pad_character) * (desired_len - len(original)))
